# -*- coding: utf-8 -*-
"""remote_sensing_classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PD0vo8VAQQ2wCVxqFzkq9Qd2vaAt6kj0

## Classification of Remotely Sensed Images
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, make_scorer, f1_score
from sklearn.model_selection import GridSearchCV
from skimage import io
from skimage.transform import resize
import cv2 as cv
import random
from math import floor, ceil

import sys, os
from google.colab import drive

IMG_CLASSES = [
    'Airport','BareLand','BaseballField','Beach','Bridge',
    'Center','Church','Commercial','DenseResidential','Desert',
    'Farmland','Forest','Industrial','Meadow','MediumResidential','Mountain',
    'Park','Parking','Playground','Pond','Port','RailwayStation','Resort','River',
    'School','SparseResidential','Square','Stadium','StorageTanks','Viaduct'
]

DRIVE_PATH = r'/content/drive/My Drive/Colab Notebooks'
DATASET_PATH = os.path.join(DRIVE_PATH, r'AID/data/')
drive.mount('/content/drive')
sys.path.append(DRIVE_PATH)

# Dataset parameters
#img_res = 150                  # Image resolution
NUM_IMGS_PER_CATEGORY = 100     # Total number of images to load from dataset
DATASET_SIZE = NUM_IMGS_PER_CATEGORY * len(IMG_CLASSES)   # Total number of images in dataset

def load_data(resolution):
  '''Load images and corresponding labels.'''
  img_data = []
  label_data = []

  # Load data from each category into array
  for i in range(len(IMG_CLASSES)):
    img_class = IMG_CLASSES[i]
    img_folder = os.path.join(DATASET_PATH, img_class)
    img_list = os.listdir(img_folder)[0:NUM_IMGS_PER_CATEGORY]
    for img_name in img_list:
      img_path = os.path.join(img_folder,img_name)
      img = np.array(io.imread(img_path))

      # Reduce image resolution and convert to 1D array
      img = resize(img,(resolution,resolution,3))
      img = img.flatten()
      img_data.append(img)

      # Append image label as number
      label_data.append(i)

  #print(img_data)
  # Shuffle images with their labels
  to_shuffle = list(zip(img_data, label_data))
  random.shuffle(to_shuffle)
  img_data, label_data = zip(*to_shuffle)
  img_data = list(img_data)
  label_data = list(label_data)

  img_data = np.array(img_data)

  # Visualize the data
  print("Data shape: {}".format(np.shape(img_data)))
  print("Label shape: {}".format(np.shape(label_data)))
  fig = plt.figure(figsize=(10,10))
  for i in range(25):
    plt.subplot(5,5,i+1)
    plt.imshow(img_data[i].reshape(resolution,resolution,3))
    plt.xticks(ticks=[])
    plt.yticks(ticks=[])
    plt.xlabel(IMG_CLASSES[label_data[i]])
  plt.suptitle("Resolution: {}x{}".format(resolution,resolution))

  return img_data, label_data


def make_dataset(resolution, amount_test=0.2):
  '''Make a training, test, and validation set from loaded data.'''
  # ratio_train is the ratio of training data to total data
  num_train = floor(DATASET_SIZE * (1 - amount_test))

  img_data, label_data = load_data(resolution)
  x_train, y_train = img_data[0:num_train], label_data[0:num_train]
  x_test, y_test = img_data[num_train:], label_data[num_train:]

  return x_train, y_train, x_test, y_test

# Save copies of dataset at three different resolutions
resolution_vals = [('low-res',50), ('mid-res',100), ('hi-res',150)]
datasets = {'low-res':None,
            'mid-res':None,
            'hi-res' :None}

for text, val in resolution_vals:
  x_train, y_train, x_test, y_test = make_dataset(val)
  dataset = {'x_train': x_train,
             'y_train': y_train,
             'x_test':  x_test,
             'y_test':  y_test}
  datasets[text] = dataset

def load_dataset_copy(dataset_name, alpha=1, beta=0):
  '''Load copy of selected dataset'''
  dataset = datasets[dataset_name]
  x_train = np.ndarray.copy(dataset['x_train'])
  y_train = np.ndarray.copy(np.array(dataset['y_train']))
  x_test = np.ndarray.copy(dataset['x_test'])
  y_test = np.ndarray.copy(np.array(dataset['y_test']))

  # Adjust image contrast and brightness (optional - must 'denormalize' data)
  x_train = cv.convertScaleAbs(x_train*255, alpha=alpha, beta=beta) / 255
  x_test = cv.convertScaleAbs(x_test*255, alpha=alpha, beta=beta) / 255

  return x_train, y_train, x_test, y_test

# Experiment 1: test SVM accuracy on different dataset resolutions
for dataset_name in ['low-res', 'mid-res', 'hi-res']:
  x_train, y_train, x_test, y_test = load_dataset_copy(dataset_name)
  svc = SVC(kernel='linear')
  svc.fit(x_train, y_train)
  y_pred = svc.predict(x_test)
  print("Classification report for {} dataset:".format(dataset_name))
  print(classification_report(y_test, y_pred))

# Experiment 2: Perform hyperparameter optimization using GridSearchCV
x_train, y_train, x_test, y_test = load_dataset_copy('low-res')
param_grid = [
    {'C':[1,10,100], 'kernel':['linear']},
    {'C':[1,10,100], 'gamma':[0.1,1,10], 'kernel':['rbf']},
    {'C':[1,10,100], 'degree':[4,5,6], 'kernel':['poly']}
]

svc = SVC()
scorer = make_scorer(f1_score, average='weighted')
hyperparam_tuning = GridSearchCV(svc,
                                 param_grid,
                                 scoring=scorer,
                                 return_train_score=True,
                                 verbose=2)
hyperparam_tuning.fit(x_train, y_train)

# After test, determine best model, params, and score
best_model = hyperparam_tuning.best_estimator_
best_params = hyperparam_tuning.best_params_
best_score = hyperparam_tuning.best_score_

print('Best model:', best_model)
print('Best parameter values:', best_params)
print('Best F1 score:', best_score)

# Put training data in form of dataframe
results = hyperparam_tuning.cv_results_
params = results['params']
scores = results['mean_test_score']
print('C  |   Gamma |   Degree  | Kernel  | Score')
i=0
for param in params:
  print(param['C'],end=",")
  if 'gamma' in param:
    print(param['gamma'],end="")
  print(',',end="")
  if 'degree' in param:
    print(param['degree'],end="")
  print(',',end="")
  print(param['kernel'],end="")
  print(',',end="")
  print(scores[i])
  i += 1

# Experiment 3: Test effect of adjusting contrast and brightness independently
#print("alpha | beta | accuracy  | f1")
for alpha, beta in [(1,0),(1.2,0),(1.5,0),(1.7,0),(2.0,0),(2.5,0),(1,-0.5),(1,-0.2),(1,0.2),(1,0.5)]:
  x_train, y_train, x_test, y_test = load_dataset_copy(dataset_name,alpha,beta)
  svc = SVC(kernel='linear')
  svc.fit(x_train, y_train)
  y_pred = svc.predict(x_test)
  print("Classification report for {},{}:".format(alpha,beta))
  print(classification_report(y_test, y_pred))

# Plot images with actual/predicted labels
  x_train, y_train, x_test, y_test = load_dataset_copy('low-res',1.5,0)
  svc = SVC(kernel='linear')
  svc.fit(x_train, y_train)
  y_pred = svc.predict(x_test)

  fig = plt.figure(figsize=(10,10))
  for i in range(25):
    plt.subplot(5,5,i+1)
    plt.imshow(x_test[i].reshape(50,50,3))
    plt.xticks(ticks=[])
    plt.yticks(ticks=[])
    plt.xlabel("{}/{}".format(IMG_CLASSES[y_test[i]], IMG_CLASSES[y_pred[i]]))